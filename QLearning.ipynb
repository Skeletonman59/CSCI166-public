{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Skeletonman59/CSCI166-public/blob/main/QLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen Lake w/ Value Iteration & Direct Evaluation"
      ],
      "metadata": {
        "id": "rTwoHidK7QXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frozen Lake Domain Description\n",
        "\n",
        "Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n",
        "\n",
        "* **S**: Starting position of the agent\n",
        "* **F**: Frozen surface, safe to walk on\n",
        "* **H**: Hole, falling into one ends the episode with a reward of 0\n",
        "* **G**: Goal, reaching it ends the episode with a reward of 1\n",
        "\n",
        "The agent can take four actions:\n",
        "\n",
        "* **0: Left**\n",
        "* **1: Down**\n",
        "* **2: Right**\n",
        "* **3: Up**\n",
        "\n",
        "However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hzTUHNC0Oien"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKf_jjk9OgN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87ca43a-c6c5-4c5f-99aa-a81afb872b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "* The agent can choose from four actions:\n",
        "    * 0: Left\n",
        "    * 1: Down\n",
        "    * 2: Right\n",
        "    * 3: Up\n",
        "\n",
        "**State Transitions:**\n",
        "\n",
        "* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n",
        "* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n",
        "    * **Successful Move:** The agent moves in the intended direction with a high probability.\n",
        "    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n",
        "* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n",
        "* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n",
        "* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R_q5-OvYOldL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()\n",
        "print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])\n",
        "\n",
        "#added code\n",
        "print (\"State 4 Going Down: (s, a, r, Done)\", env.P[4][1])\n",
        "print (\"State 11 Going Right: (s, a, r, Done)\", env.P[11][2])\n",
        "print (\"State 9 Going Right: (s, a, r, Done)\", env.P[9][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7nU_-9uaOQR",
        "outputId": "c5d4f942-1535-4ff5-c164-8e12b439dad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
            "State 4 Going Down: (s, a, r, Done) [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
            "State 11 Going Right: (s, a, r, Done) [(1.0, 11, 0, True)]\n",
            "State 9 Going Right: (s, a, r, Done) [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Iteration"
      ],
      "metadata": {
        "id": "lompMKJo2O2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "def value_iteration(env, gamma=0.9, num_iterations=1000, theta=0.0001):\n",
        "    \"\"\"\n",
        "    Implements the Value Iteration algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        gamma: Discount factor.\n",
        "        num_iterations: Number of iterations to run.\n",
        "\n",
        "    Returns:\n",
        "        The optimal value function and policy.\n",
        "    \"\"\"\n",
        "    # Initialize value function and policy\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        delta = 0\n",
        "        V_prev = V.copy()\n",
        "        for s in range(env.observation_space.n):\n",
        "            v = V[s]\n",
        "            action_values = []  # Store Q values for all actions in this state\n",
        "\n",
        "            for a in range(env.action_space.n): ##This takes every possible turn in range (up, down, left, right)\n",
        "                q_value = 0                     ##This resets q_val to 0,\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    q_value += prob * (reward + gamma * V_prev[next_state]) ##do the algo stuff to q_value\n",
        "                action_values.append(q_value)   ##save q_values to an array\n",
        "\n",
        "            # Update V[s] with the max Q value\n",
        "            V[s] = max(action_values) #push the maximum q_value into the V array\n",
        "            # Update policy[s] with the action that maximizes Q value\n",
        "            policy[s] = np.argmax(action_values) #save the specific action that led us to the max q_val (so if we had 0.3, 0.4, 0.7, 0.6, 2 would be saved, which is right)\n",
        "\n",
        "            delta = max(delta, abs(v - V[s])) # remember, v is the old value, V[s] is the one we just calculated.\n",
        "            #find absolute difference between the two, compare it to delta. delta will be replaced if abs-dif is greater\n",
        "        if delta < theta and 0: #delta is our convergence criterion. theta is our threshhold.\n",
        "            break\n",
        "        V_prev = V.copy()\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Apply student's Value Iteration\n",
        "optimal_V, optimal_policy = value_iteration(env)\n",
        "print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n optimal_V=\\n{np.round(optimal_V.reshape((4,4)), 2)}\")\n",
        "\n",
        "# Evaluate student's solution (Optional)\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _ =  env.step(action)\n",
        "            total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "average_reward = evaluate_policy(env, optimal_policy)\n",
        "print(\"Average Reward:\", average_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kv_pwvY4YR",
        "outputId": "1df7b58a-92ad-4126-f6bf-0a53f0b11b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal policy= \n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            " optimal_V=\n",
            "[[0.07 0.06 0.07 0.06]\n",
            " [0.09 0.   0.11 0.  ]\n",
            " [0.15 0.25 0.3  0.  ]\n",
            " [0.   0.38 0.64 0.  ]]\n",
            "Average Reward: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning (Lecture Slides variant, broken. See below for Gemini-fused Q-Learning"
      ],
      "metadata": {
        "id": "_aIgtsy_8CUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "while True:\n",
        "    step = global_step.eval()\n",
        "    if step >= 10000:\n",
        "        break\n",
        "    iteration +=1\n",
        "    if done:\n",
        "      observation = env.reset()\n",
        "      for skip in range(skip_start):\n",
        "        action = env.action_space.sample()  # Choose a random action\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        state = preprocess_observation(obs)\n",
        "\n",
        "    #Online DQN evaluates what to do\n",
        "    Qval = online_q_values.eval(feed_dict = {X_state: [state]})\n",
        "    action = epsilon_greedy(Qval, step)\n",
        "\n",
        "    #Online DQN plays\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = preprocess_observation(obs)\n",
        "    replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
        "    state = next_state\n",
        "\n",
        "    if iteration < training_start or iteration % training_interval != 0:\n",
        "        continue\n",
        "    X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
        "        sample_memories(batch_size))\n",
        "    next_q_values = target_q_values.eval(\n",
        "        feed_dict={X_state: X_next_state_val})\n",
        "    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
        "    y_val = rewards + continues * discount_rate * max_next_q_values\n",
        "\n",
        "    #train the online DQN\n",
        "    training_op.run(feed_dict={\n",
        "        X_state: X_state_val,\n",
        "        X_action: X_action_val,\n",
        "        y: y_val\n",
        "    })\n",
        "\n",
        "    #regularly copy the online DQN to the target DQN\n",
        "    if step % copy_steps == 0:\n",
        "        copy_online_to_target.run()\n",
        "\n",
        "    #and save regularly\n",
        "    if step % save_steps == 0:\n",
        "        online_saver.save(sess, checkpoint_path)\n",
        "\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "SRZI_Ksm8GlL",
        "outputId": "c72a6616-09a1-48c7-f5d9-713236cd548a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'global_step' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1fc2dcb8b8a7>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Take a few actions and observe the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'global_step' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q-Learning, With Gemini/GPT guidance"
      ],
      "metadata": {
        "id": "wkp0w_94CIVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n)) #initialize Q to all zeroes\n",
        "\n",
        "def choose_action(state, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return np.argmax(Q[state, :])\n",
        "\n",
        "def update_q(Q, state, action, reward, next_state, alpha, gamma):\n",
        "    best_next_action = np.argmax(Q[next_state, :])\n",
        "    td_target = reward + gamma * Q[next_state, best_next_action]\n",
        "    Q[state, action] += alpha * (td_target - Q[state, action])\n",
        "\n",
        "def q_learning(env, gamma=0.99, alpha = 0.1, epsilon = 0.3, num_episodes=100000, epsilon_decay=3.0): #approach with exploration + decay\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "          action = choose_action(state, epsilon)\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "          #Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
        "          update_q(Q, state, action, reward, next_state, alpha, gamma)\n",
        "          state = next_state\n",
        "\n",
        "          #epsilon decay check will only activate if variable is greater than 0\n",
        "          if epsilon_decay > 0.0:\n",
        "            epsilon = max(epsilon * epsilon_decay, 0.01)\n",
        "\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "    return Q, policy\n",
        "\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "Q, policy = q_learning(env)\n",
        "print (f\"optimal policy= \\n{policy.reshape((4,4))}\\n Q=\\n{Q}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQng0XzrCOUK",
        "outputId": "04d326ea-c2a4-4cb1-8eb2-635def2f381d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal policy= \n",
            "[[0 3 3 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 3 0]]\n",
            " Q=\n",
            "[[0.58489513 0.56611893 0.56903952 0.56051144]\n",
            " [0.28049902 0.42195987 0.38289544 0.52768371]\n",
            " [0.48065491 0.47177845 0.44961395 0.50653456]\n",
            " [0.39474642 0.31685229 0.35052083 0.48948051]\n",
            " [0.60078008 0.32006419 0.44309373 0.33466441]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.4105011  0.24254459 0.30611058 0.21298301]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.37236299 0.51190684 0.32529949 0.62811769]\n",
            " [0.27975067 0.65512567 0.51355667 0.41075268]\n",
            " [0.63528945 0.53127802 0.43279036 0.32535564]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.36248122 0.4217308  0.7401224  0.47272466]\n",
            " [0.73192736 0.85982725 0.8461551  0.87315733]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interesting Notes about this code box:\n",
        "\n",
        "1. When the number of episodes (run cycles) is 1000, and when the epsilon decay is 0 (therefore no exploration), we get a working optimal policy with the grid:\n",
        "\n",
        "[[1 3 0 0]\n",
        "\n",
        " [0 0 2 0]\n",
        "\n",
        " [1 2 1 0]\n",
        "\n",
        " [0 3 1 0]]\n",
        "\n",
        "However, when we increase the decay, our Q-values and policy get ruined down to zeroes. This applies to really small numbers like 0.001. Interestingly, increasing the epsilon decay to 1 gives us a policy of:\n",
        "\n",
        "[[0 3 2 3]\n",
        "\n",
        " [0 0 2 0]\n",
        "\n",
        " [3 1 2 0]\n",
        "\n",
        " [0 1 3 0]]\n",
        "\n",
        " I'll have to ask what is going on here specifically.\n",
        "\n",
        " Final Note: Actually, Increasing the decay by whole values seems to affect both policy and q-Values equally.\n",
        "\n",
        " 2. Having an epsilon of 0.3 and a decay of 0 also breaks the code.\n",
        " HOWEVER! This is not the case when you increase the number of epsiodes from 1000 to 10'000. with the same epsilon/decay, we get a good policy and q-val table printed out. Having epsilon and decay equal each other breaks the code though. This most likely shows us that the value we have for decay makes the q values converge too soon with terrible values.\n",
        "\n",
        " Further testing required.\n",
        "\n"
      ],
      "metadata": {
        "id": "1wQqZJIu42rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saved Runs:\n",
        "\n",
        "epsilon = 0.5, num_episodes = 100'000, epsilon_decay = 1.0\n",
        "\n",
        "epsilon = 0.3, num_episodes=100'000, epsilon_decay=3.0\n"
      ],
      "metadata": {
        "id": "2VQeXVs2D5an"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Extraction & Policy Evaluation"
      ],
      "metadata": {
        "id": "9Xc8WhLW2Spz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_extraction(env, V, gamma=0.9):\n",
        "  policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "  for s in range(env.observation_space.n):\n",
        "      v = V[s]\n",
        "      action_values = []  # Store Q values for all actions in this state\n",
        "\n",
        "      for a in range(env.action_space.n):\n",
        "          q_value = 0\n",
        "          for prob, next_state, reward, done in env.P[s][a]:\n",
        "              q_value += prob * (reward + gamma * V[next_state])\n",
        "          action_values.append(q_value)\n",
        "\n",
        "      # Update policy[s] with the action that maximizes Q value\n",
        "      policy[s] = np.argmax(action_values)\n",
        "  return policy\n",
        "\n",
        "def policy_evaluation(env, policy, gamma=0.9, num_iterations=1000, theta=0.0001):\n",
        "    \"\"\"\n",
        "    Implements the Value Iteration algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        gamma: Discount factor.\n",
        "        num_iterations: Number of iterations to run.\n",
        "\n",
        "    Returns:\n",
        "        The optimal value function and policy.\n",
        "    \"\"\"\n",
        "    # Initialize value function and policy\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    for _ in range(num_iterations):\n",
        "        delta = 0\n",
        "        V_prev = V.copy()\n",
        "        for s in range(env.observation_space.n):\n",
        "            v = V[s]\n",
        "            a = policy[s]\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                q_value += prob * (reward + gamma * V_prev[next_state])\n",
        "            # Update V[s] with the max Q value\n",
        "            V[s] = q_value\n",
        "\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta and 0:\n",
        "            break\n",
        "        V_prev = V.copy()\n",
        "    return V"
      ],
      "metadata": {
        "id": "XFUHBMc1kiOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_policy_evalution = policy_evaluation(env, optimal_policy)\n",
        "print (f\"V_policy_evalution=\\n{V_policy_evalution}\")\n",
        "V_policy = policy_extraction(env, V_policy_evalution)\n",
        "print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n V_policy=\\n{V_policy.reshape((4,4))}\")"
      ],
      "metadata": {
        "id": "_ZxtAYnEmeIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b163636a-43e8-4c8d-9226-5d7c812418de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V_policy_evalution=\n",
            "[0.0688909  0.06141457 0.07440976 0.05580732 0.09185454 0.\n",
            " 0.11220821 0.         0.14543635 0.24749695 0.29961759 0.\n",
            " 0.         0.3799359  0.63902015 0.        ]\n",
            "optimal policy= \n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            " V_policy=\n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning"
      ],
      "metadata": {
        "id": "4-nohV0on3fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset(seed=42)\n",
        "init_policy = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "print (f\"optimal policy= \\n{init_policy.reshape((4,4))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9uKUHB9o2dr",
        "outputId": "33a3055c-9e04-4ff6-a03f-434526100d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal policy= \n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GenerateEpisodes(env, policy, num_episodes=5):\n",
        "    total_reward = 0\n",
        "    episodes = []\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode = []\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            newstate, reward, done, _ = env.step(action)\n",
        "            episode.append((int(state), action, newstate, reward))\n",
        "            state = newstate\n",
        "        episodes.append(episode)\n",
        "    return episodes\n",
        "\n",
        "training_episodes = GenerateEpisodes(env, init_policy)\n",
        "for i, e in enumerate(training_episodes):\n",
        "  print (f\"training_episode=({i=}):\\n{e}\")\n",
        "\n",
        "training_episodes2 = GenerateEpisodes(env, init_policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htwNCLIJpo16",
        "outputId": "c9450c1a-2934-41a5-cec6-67c8ea53576f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_episode=(i=0):\n",
            "[(0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 15, 1.0)]\n",
            "training_episode=(i=1):\n",
            "[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 6, 0.0), (6, 0, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 15, 1.0)]\n",
            "training_episode=(i=2):\n",
            "[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 15, 1.0)]\n",
            "training_episode=(i=3):\n",
            "[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 6, 0.0), (6, 0, 2, 0.0), (2, 0, 1, 0.0), (1, 3, 1, 0.0), (1, 3, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0)]\n",
            "training_episode=(i=4):\n",
            "[(0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 15, 1.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct Evaluation\n",
        "\n",
        "## Evaluate Single Episode"
      ],
      "metadata": {
        "id": "f_yt-sIhrGwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EvaluateEpisode(env, e, V_DE, V_Counts, gamma=0.9):\n",
        "    future_reward = 0\n",
        "    for t in reversed(e):  # Iterate in reverse order\n",
        "        future_reward = t[3] + gamma * future_reward\n",
        "        V_DE[t[0]] = future_reward+V_DE[t[0]]\n",
        "        V_Counts[t[0]] = V_Counts[t[0]]+1\n",
        "    return V_DE, V_Counts"
      ],
      "metadata": {
        "id": "jB40XhParIyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Episode 1\n",
        "\n"
      ],
      "metadata": {
        "id": "NRQJmwWJ3E0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_DE = np.zeros((env.observation_space.n))\n",
        "V_Counts = np.zeros((env.observation_space.n))\n",
        "V_DE, V_Count = EvaluateEpisode(env, training_episodes[0], V_DE, V_Counts, 0.9)\n",
        "V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n",
        "print (f\"V_DE=\\n{V_DE.reshape((4,4))}\")\n",
        "print (f\"V_Counts=\\n{V_Counts.reshape((4,4))}\")\n",
        "print (f\"V=\\n{V.reshape((4,4))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaUV7MWJ29oe",
        "outputId": "66c223d1-213b-4841-9046-0f46918cf612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V_DE=\n",
            "[[0.59049 0.      0.      0.     ]\n",
            " [0.6561  0.      0.      0.     ]\n",
            " [0.729   0.81    0.9     0.     ]\n",
            " [0.      0.      1.      0.     ]]\n",
            "V_Counts=\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 1. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n",
            "V=\n",
            "[[0.59049 0.      0.      0.     ]\n",
            " [0.6561  0.      0.      0.     ]\n",
            " [0.729   0.81    0.9     0.     ]\n",
            " [0.      0.      1.      0.     ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-000a7187090b>:4: RuntimeWarning: invalid value encountered in divide\n",
            "  V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate All Episodes"
      ],
      "metadata": {
        "id": "wlbXCln-4EXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_DE = np.zeros((env.observation_space.n))\n",
        "V_Counts = np.zeros((env.observation_space.n))\n",
        "for e in training_episodes2:\n",
        "    V_DE, V_Count = EvaluateEpisode(env, e, V_DE, V_Counts, 0.9)\n",
        "V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n",
        "print (f\"V_DE=\\n{V_DE.reshape((4,4))}\")\n",
        "print (f\"V_Counts=\\n{V_Counts.reshape((4,4))}\")\n",
        "print (f\"V_DirectEvaluation=\\n{np.round(V.reshape((4,4)),2)}\")\n",
        "print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n optimal_V=\\n{np.round(optimal_V.reshape((4,4)), 2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OswEPertn95",
        "outputId": "241425dd-be48-4326-d40f-f6f29dbce79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V_DE=\n",
            "[[ 879.44773993   12.02033939   32.45370905    0.        ]\n",
            " [ 910.67996731    0.           63.889207      0.        ]\n",
            " [ 943.61931212  887.58499323  418.52594351    0.        ]\n",
            " [   0.         1065.13267976 1369.46029196    0.        ]]\n",
            "V_Counts=\n",
            "[[12819.   227.   503.     0.]\n",
            " [ 9779.     0.   646.     0.]\n",
            " [ 6632.  3600.  1432.     0.]\n",
            " [    0.  2876.  2159.     0.]]\n",
            "V_DirectEvaluation=\n",
            "[[0.07 0.05 0.06 0.  ]\n",
            " [0.09 0.   0.1  0.  ]\n",
            " [0.14 0.25 0.29 0.  ]\n",
            " [0.   0.37 0.63 0.  ]]\n",
            "optimal policy= \n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            " optimal_V=\n",
            "[[0.07 0.06 0.07 0.06]\n",
            " [0.09 0.   0.11 0.  ]\n",
            " [0.15 0.25 0.3  0.  ]\n",
            " [0.   0.38 0.64 0.  ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-440a84a75783>:5: RuntimeWarning: invalid value encountered in divide\n",
            "  V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n"
          ]
        }
      ]
    }
  ]
}